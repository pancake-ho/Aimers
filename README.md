# Aimers 8th: EXAONE 4.0 1.2B Model Optimization


ë³¸ ë¦¬í¬ì§€í† ë¦¬ëŠ” LG AI Researchì˜ **EXAONE 4.0 1.2B** ëª¨ë¸ì„ ê²½ëŸ‰í™”í•˜ê¸° ìœ„í•´, AutoRound ë°©ì‹ì„ ì‚¬ìš©í•œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.  
**AutoRound** ì•Œê³ ë¦¬ì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ **LoRA Fine-tuning**ì„ ê²°í•©í•˜ì—¬, ëª¨ë¸ì˜ ì„±ëŠ¥ ì €í•˜ë¥¼ ìµœì†Œí™”í•˜ë©´ì„œ ì••ì¶•ë¥ ì„ ê·¹ëŒ€í™”í•˜ëŠ” íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•˜ì˜€ìŠµë‹ˆë‹¤.


## ğŸ“Œ Project Overview


ì´ í”„ë¡œì íŠ¸ì˜ ëª©í‘œëŠ” ì œí•œëœ ë¦¬ì†ŒìŠ¤ í™˜ê²½ì—ì„œë„ EXAONE ëª¨ë¸ì´ ë†’ì€ ì„±ëŠ¥ì„ ìœ ì§€í•˜ë„ë¡ ìµœì í™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì£¼ìš” ì ‘ê·¼ ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1.  **LoRA Fine-tuning (Optional):** ì–‘ìí™” ì „, ëª¨ë¸ì´ ë°ì´í„°ì…‹(MANTA-1M)ì˜ ë¶„í¬ë¥¼ ë” ì˜ í•™ìŠµí•˜ë„ë¡ Low-Rank Adaptation(LoRA)ì„ ì ìš©í•©ë‹ˆë‹¤.
2.  **AutoRound Quantization:** Weight-only quantization ê¸°ë²•ì¸ AutoRoundë¥¼ ì‚¬ìš©í•˜ì—¬ 4-bit ì–‘ìí™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
3.  **GPTQ Format Export:** ìµœì¢… ëª¨ë¸ì„ í˜¸í™˜ì„±ì´ ë†’ì€ `auto_gptq` í¬ë§·ìœ¼ë¡œ ì €ì¥ ë° ì••ì¶•í•©ë‹ˆë‹¤.


## ğŸ› ï¸ Project Structure


```bash
Aimers/
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ config/             # ëª¨ë¸, í•™ìŠµ, ì–‘ìí™” ì„¤ì • ê´€ë¦¬
â”‚   â”œâ”€â”€ dataset/            # ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ (Train/Calib ë¶„ë¦¬)
â”‚   â”œâ”€â”€ quantizing/         # AutoRound ì•Œê³ ë¦¬ì¦˜ ì‹¤í–‰ ëª¨ë“ˆ
â”‚   â”œâ”€â”€ tuning/             # LoRA Fine-tuning ë° Merge ëª¨ë“ˆ
â”‚   â”œâ”€â”€ utils/              # ê²°ê³¼ ì €ì¥ ë° ì••ì¶• ìœ í‹¸ë¦¬í‹°
â”‚   â””â”€â”€ main.py             # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```


## âš™ï¸ Requirements


ì´ í”„ë¡œì íŠ¸ëŠ” Python 3.10.12 í™˜ê²½ì—ì„œ í…ŒìŠ¤íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤. 
ì‹¤í–‰ ì „ ì•„ë˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ì„ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.
(ê¶Œì¥: requirements.txt íŒŒì¼ì„ ìƒì„±í•˜ì—¬ pip install -r requirements.txtë¡œ ê´€ë¦¬í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.)

```bash
pip install torch transformers datasets peft trl auto-round auto-gptq
```

## ğŸš€ Usage


ì „ì²´ íŒŒì´í”„ë¼ì¸ì€ main.pyë¥¼ í†µí•´ í•œ ë²ˆì— ì‹¤í–‰ë©ë‹ˆë‹¤. (ìš°ë¶„íˆ¬)
ì‹¤í–‰ì´ ì™„ë£Œë˜ë©´ model/ ë””ë ‰í† ë¦¬ì— ì–‘ìí™”ëœ ëª¨ë¸ íŒŒì¼ê³¼ ì œì¶œìš© zip íŒŒì¼ì´ ìƒì„±ë©ë‹ˆë‹¤.

```bash
(ë ˆí¬ì§€í† ë¦¬ í´ë¡ )
git clone [https://github.com/pancake-ho/Aimers.git](https://github.com/pancake-ho/Aimers.git)
cd Aimers/model

(íŒŒì´í”„ë¼ì¸ ì‹¤í–‰)
python3 main.py --num_train (ìˆ«ì) --tuning (True or False) --num_calib (ìˆ«ì)
```

## ğŸ§ª Methodology Details


**1. Fine-tuning Stage**

Method: LoRA (Low-Rank Adaptation)

Library: peft, trl

Details: q_proj, k_proj, v_proj ë“± ëª¨ë“ˆì— ì–´ëŒ‘í„°ë¥¼ ë¶€ì°©í•˜ì—¬ ì†Œê·œëª¨ ë°ì´í„°ë¡œ ë¹ ë¥´ê²Œ í•™ìŠµí•œ ë’¤, ì›ë³¸ ëª¨ë¸ì— Mergeí•©ë‹ˆë‹¤.


**2. Quantization Stage**

Method: AutoRound (Advanced Weight-Rounding)

Bits: 4-bit

Group Size: 128

Algorithm: Calibration ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ Layerë³„ ìµœì ì˜ Weight Roundingì„ í•™ìŠµí•©ë‹ˆë‹¤.
