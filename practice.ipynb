{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39cc19a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 일부: tensor([-0.2172,  0.6953,  0.0411,  2.3066, -0.1843], dtype=torch.float16)\n",
      "양자화 적용본: tensor([-0.3296,  0.6592,  0.0000,  2.3066, -0.3296], dtype=torch.float16)\n",
      "오차(MSE): 0.007106781005859375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def psuedo_quant_tensor(w, n_bit=4, group_size=128):\n",
    "    \"\"\"\n",
    "    실제 int4 로 변환하지 않고, float16 상태에서 값만 4bit 스타일로 뭉개는 함수\n",
    "    PPL 을 빠르게 예측하는 데 유리\n",
    "    \"\"\"\n",
    "    org_w_shape = w.shape\n",
    "\n",
    "    # 그룹 단위로 reshape\n",
    "\n",
    "    # scale 계산\n",
    "    # 각 채널 별 절댓값 최대치 찾기\n",
    "    max_val = w.abs().amax(dim=1, keepdim=True)\n",
    "    max_val = max_val.clamp(min=1e-5)\n",
    "\n",
    "    # 4bit 의 최대 표현 범위 지정\n",
    "    max_int = 2 ** (n_bit - 1) - 1\n",
    "    min_int = - (2 ** (n_bit - 1))\n",
    "    scale = max_val / max_int\n",
    "\n",
    "    w_quant = torch.clamp(torch.round(w / scale), min=min_int, max=max_int)\n",
    "\n",
    "    # 역양자화 과정 (다시 복원해보면 정보 손실을 확인할 수 있음)\n",
    "    w_dequant = w_quant * scale\n",
    "\n",
    "    return w_dequant, scale\n",
    "\n",
    "# 테스트\n",
    "dummy_weight = torch.randn(10, 10, dtype=torch.float16)\n",
    "q_weight, s = psuedo_quant_tensor(dummy_weight)\n",
    "\n",
    "print(f\"원본 일부: {dummy_weight[0][:5]}\")\n",
    "print(f\"양자화 적용본: {q_weight[0][:5]}\")\n",
    "print(f\"오차(MSE): {(dummy_weight - q_weight).pow(2).mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8579de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 모델 로드 (EXAONE 4.0 1.2B)\n",
    "model_id = \"LGAI-EXAONE/EXAONE-4.0-1.2B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# 샘플 데이터 투입\n",
    "input_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "\n",
    "def layer_sensitivity(model, inputs):\n",
    "    \"\"\"\n",
    "    모델 레이어의 양자화 민감도를 측정하는 함수\n",
    "    \"\"\"\n",
    "    sensitivities = {}\n",
    "    print(\"모델 레이어별 민감도 측정을 시작합니다.\")\n",
    "\n",
    "    layers = model.model.layers\n",
    "\n",
    "    for i, layer in enumerate(tqdm(layers)):\n",
    "\n",
    "        # 1. 원본 레이어의 출력값 뽑아내기\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
